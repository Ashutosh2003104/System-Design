Click(To read Complete Article): (https://ashutoshroy104.substack.com/p/cap-theorem-guide-consistency-availability)

# CAP Theorem Guide: Consistency, Availability, and Partition Tolerance (Basic to Advance)

### "Why you have to choose between Consistency and Availability (and why Partition Tolerance is not a choice)."

---

## Start with a real-life story

Imagine **WhatsApp group chat** with 3 friends:

* You (India)
* Friend A (USA)
* Friend B (UK)

Messages are stored on **multiple servers** so everyone gets messages fast.

Now think carefully about **what can go wrong**.

### Situation:

You send a message:

> “Meeting at 10 AM”

Suddenly, the **network cable between India ↔ USA breaks**.

Now the system must decide:

* Should USA see the message?
* Should UK see it?
* Should the system stop or continue?

This is where **CAP Theorem lives**.

---

# Pre-requisite (For better understanding)

## What is a Distributed System?

Before CAP, one definition is critical.

### Distributed System:

A system where **data is stored on multiple machines (nodes)** that communicate over a **network**.

Examples:

* WhatsApp servers
* Amazon databases
* Google Search
* Netflix backend

Once **network** is involved, failures are **guaranteed**.

### Example

#### **Microservices**

This is the modern standard for large tech companies. Instead of one giant program doing everything, the application is broken into hundreds of tiny, independent “services” that talk to each other.

* *Example:* When you buy something on Amazon, one service handles the “Buy” button, another checks “Inventory,” and a third handles “Shipping.” If the “Recommendations” service crashes, you can still buy items.

---

# Main Concept (CAP)

## **C - Consistency**

In the world of distributed systems, **Consistency** is the answer to a single, critical question:

> *“When I update data on one computer, how long until everyone else sees that update?”*

If a distributed system behaves exactly like a single, unified computer—where a change happens instantly for everyone—it has high consistency. If different users see different versions of the data at the same time, it has low consistency.

Here is a detailed breakdown of how it works, the trade-offs involved, and a concrete example.

### 1. The Core Problem: Replication

To understand consistency, you must understand **Replication**. We rarely store data on just one server. We copy (replicate) it across multiple servers (Node A, Node B, Node C) so that if Node A catches fire, the data survives on B and C.

**The conflict:** When you update Node A, it takes time (latency) for that update to travel to Node B and C. During that travel time, the system is **inconsistent**. Node A has the new truth; Nodes B and C have the old truth.

### 2. The Two Main Types of Consistency

Engineers usually have to choose between two extremes.

#### **A. Strong Consistency (The “Strict” Approach)**

This model guarantees that once a write is confirmed, **all** future reads will see that new data.

* **How it works:** When you update data on Node A, the system “locks” the data. It sends the update to B and C. It will not confirm to you that the save is finished until B and C acknowledge they have received it.
* **The Cost:** It is **slow**. If Node C is having network issues, the whole system freezes and waits.
* **Best for:** Banking, Medical records, Inventory management.

#### **B. Eventual Consistency (The “Fast” Approach)**

This model guarantees that if no new updates are made, **eventually** all accesses will return the last updated value.

* **How it works:** When you update Node A, it says “Success!” immediately. It then quietly updates B and C in the background (asynchronously). For a few seconds (or milliseconds), a user connected to Node B will still see the old data.
* **The Benefit:** It is **extremely fast** and robust. Even if Node C is offline, Node A keeps working.
* **Best for:** Social media feeds, YouTube view counts, Review sections.

### 3. The Concrete Example: “The Last Ticket”

Imagine a concert ticketing website with two servers: **Server New York** and **Server London**. Both share a database of available seats. There is exactly **one** ticket left for a Taylor Swift concert.

#### **Scenario 1: Using Strong Consistency**

1. **User A (in NY)** clicks “Buy” on the last ticket.
2. **Server NY** receives the request. It tells Server London: *“Stop! Do not sell this ticket. I am processing it.”*
3. **User B (in London)** clicks “Buy” 0.1 seconds later.
4. **Server London** checks and sees the “Stop/Lock” signal from NY. It tells User B: *“Please wait...”* or *“Checking inventory...”*
5. **Server NY** completes the sale and deletes the ticket from the database. It tells Server London: *“Okay, the ticket is gone.”*
6. **Server London** tells User B: *“Sorry, sold out.”*
* **Result:** Correctness is preserved. Only one person got the ticket.
* **Downside:** User B had to wait (latency) while the servers synchronized.

#### **Scenario 2: Using Eventual Consistency**

1. **User A (in NY)** clicks “Buy” on the last ticket.
2. **Server NY** immediately processes the sale and says *“Congrats! You got it.”* It queues a message to tell Server London, but it doesn’t wait.
3. **User B (in London)** clicks “Buy” 0.1 seconds later.
4. **Server London** has not received the message yet. According to its local data, the ticket is still there. It processes the sale and says *“Congrats! You got it.”*
5. **Later:** The servers sync up and realize they sold the same seat twice.
6. **Conflict Resolution:** The business logic kicks in. Perhaps the system cancels User B’s ticket and sends an apology email.
* **Result:** The system was fast for everyone, but created a business conflict (double booking) that has to be fixed later.
* **Upside:** No one had to wait, and the system worked even if the connection between NY and London was slow.

---

## **A - Availability**

**Availability** in a distributed system answers this simple question:

> *“If I try to use the system, will it work right now?”*

Formally, Availability guarantees that **every request receives a response** (success or failure), without the guarantee that it contains the most recent version of the data. It means the system is “online” and functional, even if some parts of it are broken or undergoing maintenance.

Here is the detailed breakdown.

### 1. The Core Concept: “Always On”

In a high-availability system, the priority is **uptime**. If a user clicks a button, they must see a result. They should never see a “Connection Timed Out” or “500 Internal Server Error” page.

To achieve this, the system must eliminate “Single Points of Failure.” If the system relies on one specific database and that database crashes, the system is unavailable. To fix this, we use:

* **Redundancy:** having multiple copies of the server (Node A, Node B, Node C).
* **Failover:** If Node A dies, the system automatically redirects traffic to Node B.

### 2. How we Measure it: “The Nines”

Availability is measured in the percentage of time the system is operational over a year.

### 3. The Trade-off: Accuracy vs. Uptime

This brings us back to the **CAP Theorem**. Often, to guarantee Availability, you must sacrifice Consistency.

If the connection between your US and EU servers breaks:

* **Choosing Consistency:** You shut down the EU server so it doesn’t show old data. (Result: **System Unavailable** for EU users).
* **Choosing Availability:** You keep the EU server running, even though it might show data that is 5 minutes old. (Result: **System Available**, but data is inconsistent).

### 4. Concrete Example: The “Black Friday” Shopping Cart

Imagine you are running a massive e-commerce store (like Amazon) during a Black Friday sale. Traffic is spiking, and thousands of people are adding items to their carts every second.

Suddenly, the **Main Inventory Database** (which tracks exactly how many items are left) becomes overloaded and stops responding.

#### **Scenario A: Low Availability (Prioritizing Consistency)**

1. A user clicks “Add to Cart” on a TV.
2. The web server tries to ask the Inventory Database: *“Do we have this TV in stock?”*
3. The database does not respond.
4. The web server times out and shows the user: **“Error: Service Temporarily Unavailable. Please try again.”**
5. **Result:** The user gets frustrated, leaves, and buys the TV from a competitor. **You lose money.**

#### **Scenario B: High Availability (Prioritizing Uptime)**

1. A user clicks “Add to Cart” on a TV.
2. The web server tries to ask the Inventory Database. It gets no response.
3. **The Failover Logic:** Instead of showing an error, the system says, *“I can’t check stock right now, but I will accept the order anyway.”*
4. The item is added to the cart. The user checks out and pays.
5. **Result:** The user is happy. The system stayed “Available” despite a critical failure.
6. **The Risk:** Later, when the database comes back online, you might realize you sold 105 TVs but only had 100 in stock. You now have to email 5 customers to apologize and refund them.

	* *Business Decision:* It is better to process 10,000 orders and apologize to 5 people than to block all 10,000 orders because the database was slow.

### 5. Key Technologies for Availability

To build highly available systems, engineers use:

* **Load Balancers:** (e.g., NGINX, AWS ELB) These sit in front of your servers. If Server A crashes, the load balancer detects it and instantly sends users to Server B.
* **Replication:** Storing data in multiple data centers (e.g., US-East and US-West). If an earthquake hits the East Coast, the West Coast takes over.
* **Circuit Breakers:** A software pattern that detects if a service is failing and stops sending requests to it, returning a “cached” or “default” response instead of crashing the whole system.

---

## **P - Partition Tolerance**

**Partition Tolerance** is the ability of a distributed system to keep working even if the network connecting the computers breaks.

In a distributed system, you have Node A and Node B talking to each other. A “Partition” happens when the communication line between them is cut (a router crashes, a cable is severed, or the Wi-Fi drops). Node A is still alive, and Node B is still alive, but they cannot talk to each other.

**Partition Tolerance guarantees:** The system does not crash just because the nodes can’t talk. It continues to operate, even if it has to make difficult compromises.

### 1. The “Non-Negotiable” Reality

Unlike Consistency (C) and Availability (A), **Partition Tolerance (P) is not really a choice.**

If you are building a distributed system over a physical network (like the Internet), partitions **will** happen.

* Undersea cables get cut by sharks.
* Cloud providers (AWS/Azure) have outages.
* Routers get misconfigured.

Since you cannot guarantee a perfect network 100% of the time, your system **must** be Partition Tolerant. If it isn’t, your entire system goes down every time a minor network glitch occurs.

### 2. The Consequence: The “Split Brain”

When a partition occurs, your system is effectively sliced into two isolated islands.

* **Island 1 (Node A):** Can talk to Users in New York.
* **Island 2 (Node B):** Can talk to Users in London.
* **The Problem:** Node A and Node B cannot talk to *each other* to sync data.

This is where the **CAP Theorem** forces you to make a hard decision. Since the partition has happened (P), you must now choose between **Consistency (C)** or **Availability (A)** until the network is fixed.

### 3. Concrete Example: The Split ATM

Imagine a bank with two ATMs: **ATM-North** and **ATM-South**. They are connected by a network cable and share one database. You have **$500** in your account.

Suddenly, **a construction worker accidentally cuts the cable** connecting the two ATMs. The network is partitioned.

#### **Scenario 1: The “CP” Approach (Consistency + Partition Tolerance)**

The bank decides: *“We value accuracy above all else.”*

1. You walk up to **ATM-North** and try to withdraw $100.
2. ATM-North tries to check with ATM-South to make sure you didn’t already withdraw money there.
3. It realizes the cable is cut (Partition).
4. **The Decision:** ATM-North displays **“Out of Service”** and refuses to give you money.
5. **Result:** The system is **Unavailable** (you are annoyed), but it is **Consistent** (no risk of overdraft).

#### **Scenario 2: The “AP” Approach (Availability + Partition Tolerance)**

The bank decides: *“Customers must always be able to get cash.”*

1. You walk up to **ATM-North** and withdraw $500.
2. ATM-North cannot talk to ATM-South, but it gives you the cash anyway to stay **Available**.
3. Simultaneously, your partner walks up to **ATM-South** and tries to withdraw $500.
4. ATM-South has no idea you just emptied the account at ATM-North. It gives your partner $500 too.
5. **Result:** The system stayed **Available** (you are happy), but it is **Inconsistent** (the bank just lost $500 because the account is now -$500).

### 4. How Engineers Handle Partitions

Since we cannot prevent partitions, we design systems to recover from them.

#### **If you chose AP (Availability):**

You accept that data will be messy during the partition. When the network comes back online, you perform **Reconciliation**.

* *Example:* In the ATM scenario, once the cable is fixed, the bank’s computers talk, realize the account is negative, and perhaps charge you an overdraft fee or block your card.
* *Real World:* This is how Google Docs works. If you go offline (partition) and type a sentence, and your friend types a different sentence, Google Docs merges them when you come back online.

#### **If you chose CP (Consistency):**

You accept downtime. You usually employ a **Quorum**.

* *Example:* If you have 5 servers and the network splits them into a group of 3 and a group of 2, the group of 3 (the majority) keeps working, and the group of 2 shuts down completely to prevent data corruption.
